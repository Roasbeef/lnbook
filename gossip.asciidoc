# Channel Graph Discovery, Authentication & Maintenance

## Intro

As we have seen the Lightning Network uses a sourced base Onion Routing Protocol to deliver a payment from a sender to the recipient.
For this the sending node has to be able to construct a path of payment channels that connects it with the recipient.
Thus the sender has to be aware of the Channel Graph.
The channel graph is the interconnected set of publicly advertised channels (more on that later), and the nodes that these channels interlink.
As channels are backed by a funding transaction that is happening on chain one might falsely believe that Lightning Nodes could just extract the existing channels from the Bitcoin Blockchain.
However this only possible to a certain extend.
The Funding transactions are Pay to Whitness Script addresses and the nature of the script will only be revealed once the funding transaction output is spent.
Even if the nature of the script was known we emphasize that not all 2 - of - 2 multisignature scripts correspond to payment channels.
Since not even every Pay to Whitness Script address that we see in the Bitcoin Blockchain corresponds to a payment channel this approach is not helpful.
There are actually more reasons why looking at the Bitcoin Blockchain might not be helpful.
For example on the Lightnign Network the Bitcoin keys that are used for signing are rotated by the nodes for every channel and update.
Thus even if we could reliably detect funding transactions on the Bitcoin Blockchain we would not know which two nodes on the Lightning Network own that particular channel.
Thus we could node create the Channel Graph that would be used to create candidate paths during the payment delivery via onion routing.

The Lightning Network solves the afore mentioned problem by implementing a gossip protocol.
Gossip protocols are typical for peer 2 peer networks and are being used so that nodes can share information with other nodes without talking to those other nodes directly.
For that Lightning Nodes open encrypted peer 2 peer connections to each other and share novel information that they have received from other peers.
As soon as a node wants to share some information - for example about a newly created channel - it sends a message to all its peers.
Uppon receiving a message a node decides if the received message was novel and if so it will forward the information to its peers.
In this way if the peer 2 peer network is connected all new information that is necessary for the operation of the network will eventually be propagated to all other peers. 

Obviously if a new peer initially wants to join the network it needs to know some other peers on the Network initially.
Only then the new peer could connect to them in order to learn about the existance of other peers. 
In this chapter, we'll explore exactly _how_ Lightning nodes discover each
other, and also the channel graph itself.
When most refer to the _network_ part
of the Lightning Network, they're referring to the Channel Graph which itself
is a unique authenticated data structure _anchored_ in the base Bitcoin
blockchain.
However the Lightning network is also a peer to peer network of nodes that are have just opened an encrypted peer connection.
Usually for 2 peers to maintain a payment channel they need to talk to each other directly which means that there will be a peer connection between them.
This suggests that the Channel Graph is a subnetwork of the peer to peer network.
However this is not true.
As payment channels do not need to be closed if one or both peers are going offline they can continue to exist even though the peer connection is terminated.
It might be usefull to get familiar with the different terminology that we have used througout the book for similar concepts / actions depending on weather they happen on the Channel Graph or on the peer 2 peer network.

[[network-terminology]]
.Terminology of the different Networks
[options="header"]
|===
|  | Channel Graph  |peer to peer network
| Name     | channel | connection 
| initiate | open | (re)connect 
| finalize | close| terminate
| technology | Bitcoin Blockchain | encrypted TCP/IP connection
| lifetime | until funding spent | while peers are online
|===

As the Lightning Network is a peer-to-peer network, some initial bootstrapping
is required in order for peers to discover each other.  Within this chapter
we'll follow the story of a new peer connecting to the network for the first
time, and examine each step in the bootstrapping process from initial peer
discovery, to channel graph syncing and validation.

As an initial step, our new node needs to somehow _discover_ at least _one_
peer that is already connected to the network and has a full channel graph (as
we'll see later, there's no canonical version as the update dissemination
system is _eventually consistent_). Using one of many initial bootstrapping
protocols to find that first peer, after a connection is established, our new
peer now needs to _download_ and _validate_ the channel graph. Once the channel
graph has been fully validated, our new peer is ready to start opening channels
and sending payments on the network. 

After initial bootstrap, a node on the network needs to continue to maintain
its view of the channel graph by: processing new channel routing policy
updates, discovering and validating new channels, removing channels that have
been closed on chain, and finally pruning channels that fail to send out a
proper "heartbeat" every 2 weeks or so.

Upon completion of this chapter, the reader will understand a key component of
the p2p Lightning Network: namely how peers discover each other and maintain a
personal view of the channel graph. We'll being our story with exploring the
story of a new node that has just booted up, and needs to find other peers to
connect to on the network.

## Peer Discovery

In this section, we'll begin to follow the story of Norman, a new Lightning
node that wishes to join the network as he sets out on his journey to which consists of 3 steps:

. discovery a set of bootstrap peers.
. download and validate the channel graph.
. begin the process of ongoing maintainance of the channel graph itself.


### P2P Boostrapping

Before doing any thing else, Norman first needs to discover a set of peers whom
are already part of the network. We call this process initial peer
bootstrapping, and it's something hat every peer to peer network needs to
implement properly in order to ensure a robust, healthy network. Bootstrapping
new peers to existing peer to peer networks is a very well studied problem with
several known solutions, each with their own distinct trade offs. The simplest
solution to this problem is simply to package a set of _hard coded_ bootstrap
peers into the packaged p2p node software. This is simple in that each new node
can find the bootstrap peer with nothing else but the software they're running,
but rather fragile given that if the set of bootstrap peers goes offline, then
no new nodes will be able to join the network. Due to this fragility, this
option is usually used as a fallback in case none of the other p2p bootstrapping
mechanisms work properly.

Rather than hard coding the set of bootstrap peers within the software/binary
itself, we can instead opt to devise a method that allows peers to dynamically
obtain a fresh/new set of bootstrap peers they can use to join the network.
We'll call this process initial peer discovery. Typically we'll leverage
existing Internet protocols to maintain and distribute a set of bootstrapping
peers. A non-exhaustive list of protocols that have been used in the past to
accomplish initial peer discovery include:

  * DNS
  * IRC
  * HTTP

Similar to the Bitcoin protocol, the primary initial peer discovery mechanism
used in the Lightning Network happens via DNS. As initial peer discovery is a critical and
universal task for the network, the process has been _standardized_ in a
document that is a part of the Basis of Lightning Technology (BOLT)
specification. This document is [BOLT
10](https://github.com/lightningnetwork/lightning-rfc/blob/master/10-dns-bootstrap.md).

### DNS Bootstrapping via BOLT 10

The BOLT 10 document describes a standardized way of implementing peer
discovery using the Domain Name System (DNS). Lightning's flavor of DNS based
bootstrapping uses up to 3 distinct record types:

  * `SRV` records for discovering a set of _node public keys_.
  * `A` records for mapping a node's public key to its current `IPv4` address.
  * `AAA` records for mapping a node's public key to its current `IPv6` address
   (if one exists).

Those somewhat familiar with the DNS protocol may already be familiar with the
`A` and `AAA` record types, but not the `SRV` type. The `SRV` record type is
used less commonly by protocols built on DNS, that's used to determine the
_location_ for a specified service. In our context, the service in question is
a given Lightning node, and the location its IP address. We need to use this
additional record type as unlike nodes within the Bitcoin protocol, we need
both a public key _and_ an IP address in order to connect to a node. As we saw
in chapter XX on the transport encryption protocol used in LN, by requiring
knowledge of the public key of a node before one can connect to it, we're able
to implement a form of _identity_ hiding for nodes in the network.

// TODO(roasbeef): move paragraph below above?

#### A New Peer's Bootstrapping Workflow

Before diving into the specifics of BOLT 10, we'll first outline the high level
flow of a new node that wishes to use BOLT 10 to join the network. 

First, a node needs to identify a single, or set of DNS servers that understand
BOLT 10 so they can be used for p2p bootstrapping.
While BOLT 10 uses lseed.bitcoinstats.com as the seed server as the example there exists no "official"
set of DNS seeds for this purpose, but each of the major implementations
maintain their own DNS seed, and cross query each other's seeds for redundancy
purposes.

[[dns seeds]]
.Table of known lightning dns seed servers
[options="header"]
|===
| dns server     | Maintainer 
| lseed.bitcoinstats.com | Christian Decker
| nodes.lightning.directory | lightning labs (Olaoluwa Osuntokun)
| soa.nodes.lightning.directory | lightning labs (Olaoluwa Osuntokun)
| lseed.darosior.ninja | Antoine Poinsot
|===


DNS seeds exist for both Bitcoin's mainnet and testnet. For the sake
of our example, we'll assume the existence of a valid BOLT 10 DNS seed at
`nodes.lightning.directory`.

Next, we'll now issue an `SRV` query to obtain a set of _candidate bootstrap
peers_. The response to our query will be a series of _bech32_ encoded public
keys. As DNS is a text based protocol, we can't send raw bytes, so an encoding
scheme is required. For this scheme BOLT 10 has selected _bech32_ due to its
existing propagation in the wider Bitcoin ecosystem. The number of encoded
public keys returned depends on the server returning the query, as well as all
the resolver that stand between the client and the authoritative server. Many
resolvers may filter out SRV records all together, or attempt to truncate the
response size itself.

Using the widely available `dig` command-line tool, we can query the _testnet_
version of the DNS seed mentioned above with the following command: 
```
$ dig @8.8.8.8 test.nodes.lightning.directory SRV
```

We use the `@` argument to force resolution via Google's nameserver as they
permit our larger SRV query responses. At the end, we specify that we only want
`SRV` records to be returned. A sample response looks something like:
```
$ dig @8.8.8.8 test.nodes.lightning.directory SRV

; <<>> DiG 9.10.6 <<>> @8.8.8.8 test.nodes.lightning.directory SRV
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 43610
;; flags: qr rd ra; QUERY: 1, ANSWER: 25, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;test.nodes.lightning.directory.	IN	SRV

;; ANSWER SECTION:
test.nodes.lightning.directory.	59 IN	SRV	10 10 9735 ln1qfkxfad87fxx7lcwr4hvsalj8vhkwta539nuy4zlyf7hqcmrjh40xx5frs7.test.nodes.lightning.directory.
test.nodes.lightning.directory.	59 IN	SRV	10 10 15735 ln1qtgsl3efj8verd4z27k44xu0a59kncvsarxatahm334exgnuvwhnz8dkhx8.test.nodes.lightning.directory.

<SNIP>

;; Query time: 89 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Thu Dec 31 16:41:07 PST 2020
```

We've truncated the response for brevity. In our truncated responses, we can
see two responses. Starting from the right-most column, we have a candidate
"virtual" domain name for a target node, then to the left we have the _port_
that this node can be reached using. The first response uses the standard port
for LN: `9735`. The second response uses a custom port which is permitted by
the protocol.

Next, we'll attempt to obtain the other piece of information we need to connect
to a node: it's IP address. Before we can query for this however, we'll fist
_decode_ the returned sub-domain for this virtual host name returned by the
server. To do that, we'll first encoded public key:
```
ln1qfkxfad87fxx7lcwr4hvsalj8vhkwta539nuy4zlyf7hqcmrjh40xx5frs7
```

Using `bech32`, we can decode this public key to obtain the following valid
`secp256k1` public key:
```
026c64f5a7f24c6f7f0e1d6ec877f23b2f672fb48967c2545f227d70636395eaf3
```

Now that we have the raw public key, we'll now ask the DNS server to _resolve_
the virtual host given so we can obtain the IP information for the node:
```
$ dig ln1qfkxfad87fxx7lcwr4hvsalj8vhkwta539nuy4zlyf7hqcmrjh40xx5frs7.test.nodes.lightning.directory A

; <<>> DiG 9.10.6 <<>> ln1qfkxfad87fxx7lcwr4hvsalj8vhkwta539nuy4zlyf7hqcmrjh40xx5frs7.test.nodes.lightning.directory A
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 41934
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;ln1qfkxfad87fxx7lcwr4hvsalj8vhkwta539nuy4zlyf7hqcmrjh40xx5frs7.test.nodes.lightning.directory. IN A

;; ANSWER SECTION:
ln1qfkxfad87fxx7lcwr4hvsalj8vhkwta539nuy4zlyf7hqcmrjh40xx5frs7.test.nodes.lightning.directory. 60 IN A X.X.X.X

;; Query time: 83 msec
;; SERVER: 2600:1700:6971:6dd0::1#53(2600:1700:6971:6dd0::1)
;; WHEN: Thu Dec 31 16:59:22 PST 2020
;; MSG SIZE  rcvd: 138
```

In the above command, we've queried the server so we can obtain an `IPv4`
address for our target node. Now that we have both the raw public key _and_ IP
address, we can connect to the node using the `brontide` transport protocol at:
`026c64f5a7f24c6f7f0e1d6ec877f23b2f672fb48967c2545f227d70636395eaf3@X.X.X.X`.
Querying for the curent `A` record for a given node can also be used to look up
the _latest_ set of addresses for a given node. Such queries can be used to
more quickly (compared to waiting on gossip as we'll cover later) sync the
latest addressing information for a node.

At this point in our journey, Norman our new Lightning Node has found its first
peer and established its first connect! Now we can being the second phase of
new peer bootstrapping: channel graph synchronization and validation, but
first, we'll explore more of the intricacies of BOLT 10 itself to take a deeper
look into how things work under the hood.

### A Deep Dive Into BOLT 10

As we learned earlier in the chapter, BOLT 10 describes the standardized
protocol for boostrapping new peer suing the DNS protocol. In this section,
we'll dive into the details of BOLT 10 itself in order to explore exactly how
bootstrapping queries are made, and also the additional set of options
available for querying.

#### SRV Query Options

The BOLT 10 standard is highly extensible due to its usage of nested
sub-domains as a communication layer for additional query options. The
bootstrapping protocol allows clients to further specify the _type_ of nodes
they're attempting to query for vs the default of receiving a random subset of
nodes in the query responses.

The query option sub-domain scheme uses a series of key-value pairs where the
key itself is a _single letter_ and the remaining set of text is the value
itself. The following query types exist in the current version of the BOLT 10
standards document:

  * `r`: The "realm" byte which is used to determine which chain or realm
    queries should be returned for. As is, the only value for this key is `0`
    which denotes Bitcoin itself.

  * `a`: Allows clients to filter out returned nodes based on the _types_ of
    addresses they advertise. As an example, this can be used to only obtain
    nodes that advertise a valid IPv6 address.
      * The value that follows this type is based on a bitfled that _indexes_
        into the set of specified address _type_ which are defined in BOLT 7.
        We'll cover this material shortly later in this chapter once we examine
        the structure of the channel graph itself.
      * The default value for this field is `6`, which is `2 || 4`, which denotes
        bit 1 and 2, which are IPv4 and IPv6.

  * `l`: A valid node public key serialized in compressed format. This allows a
    client to query for a specified node rather than receiving a set of random
    nodes.

  * `n`: The number of records to return. The default value for this field is
   `25`.

An example query with additional query options looks something like the following: 
```
r0.a2.n10.nodes.lightning.directory
```

Breaking down the query one key-value pair at a time we gain the following
insights:

  * `r0`: The query targets the Bitcoin realm.
  * `a2`: The query only wants IPv4 addresses to be returned.
  * `n10`: The query requests  

## Channel Graph: Structure and Attributes

Now that Norman is able to use the DNS boostrapping protocol to connect to his
very first peer, we can now start to sync the channel graph! However, before we
sync the channel graph, we'll need to learn exactly _what_ we mean by the
channel graph. In this section we'll explore the precise _structure_ of the
channel graph and examine the unique aspects of the channel graph compared to
the typical abstract "graph" data structure which is well known/used in the
field of Computer Science.

### The Channel Graph as a Directed Overlay Data Structure

A graph in computer science is a special data structure composed of vertices
(typically referred to as nodes) and edges (also known as links). Two nodes may
be connected by one or more edges. The channel graph is also _directed_ given
that a payment is able to flow in either direction over a given edge (a
channel). As we're concerned with _routing payments_, in our model a node with
no edges isn't considered to be a part of the graph as it isn't "productive".
In the context of the Lightning Network, our vertices are the Lightning nodes
themselves, with our edges being the channels that _connect_ these nodes. As
channels are themselves a special type of multi-sig UTXO anchored in the base
Bitcoin blockchain, possible to scan the chain (with the assistance of special
meta data proofs) and re-derive the channel graph first-hand (though we'd be
missing some information as we see below).

As channels themselves are UTXOs, we can view the channel graph as a special
sub-set of the UTXO set, on top of which we can add some additional information
(the nodes, etc) to arrive at the final overlay structure which is the channel
graph. This anchoring of fundamental components of the cahnnel graph in the
base Bitcoin blockchain means that it's impossible to _fake_ a valid channel
graph, which has useful properties when it comes to spam prevention as we'll
see later. The channel graph in the Lightning Network is composed of 3
individual components which are described in BOLT 7:
 
 * `node_announcement`: The vertex in our graph which communicates the public
   key of a node, as well as how to reach the node over the internet and some
   additional metadata describing the set of _features_ the node supports.

 * `channel_announcement`: A blockchain anchored proof of the existence of a
   channel between two individual nodes. Any 3rd party can verify this proof in
   order to ensure that a _real_ channel is actually being advertised. Similar
   to the `node_announcement` this message also contains information describing
   the _capabilities_ of the channel which is useful when attempting to route a
   payment.

 * `channel_update`: A _pair_ of structures that describes the set of _routing
   policies_ for a given channel. `channel_update`s come in a _pair_ as a
   channel is a directed edge, so both sides of the channel are able to specify
   their own custom routing policy. An example of a policy communicated in a 

It's important to note that each of components of the channel graph are
themselves _authenticated_ allowing a 3rd party to ensure that the owner of a
channel/update/node is actually the one sending out an update. This effectively
makes the Channel Graph a unique type of _authenticated data structure_ that
cannot be counterfeited. For authentication, we use an `secp256k1` ECDSA
digital signature (or a series of them) over the serialized digest of the
message itself. We won't get into the specific of the messaging
framing/serialization used in the LN in this chapter, as we'll cover that
information in Chapter XX on the wire protocol used in in the protocol.

With the high level structure of the channel graph laid out, we'll now dive
down into the precise structure of each of the 3 components of the channel
graph. We'll also explain how one can also _verify_ each component of the
channel graph as well.

#### Node Announcement: Structure & Validation

First, we have the `node_announcement` which plays the role of the vertex in
the channel graph. A node's announcement in the network serves to primary
purposes:

 1. To advertise connection information so other nodes can connect to it,
 either to bootstrap to the network, or to attempt to establish a set of new
 channels.

 2. To communicate the set of features protocol level features a node
 understands. This communication is critical to the decentralized
 de-synchronized update nature of the Lightning Network itself.

Unlike channel announcements, node announcements aren't actually anchored in
the base blockchain. As a result, advertising a node announcement in isolation
bares no up front cost. As a result, we require that all node announcements are
only considered "valid" if it has propagated with a corresponding channel
announcement as well. In other words, we always reject unconnected nodes in
order to ensure a rogue peer can't fill up our disk with bogus nodes that may
not actually be part of the network.

##### Structure

The node announcement is a simple data structure that needs to exist for each
node that's a part of the channel graph. The node announcement is comprised of
the following fields, which are encoded using the wire protocol structure
described in BOLT 1:

  * `signature`: A valid ECDSA signature that covers the serialized digest of
    all fields listed below. This signature MUST be venerated by the private
    key that backs the public key of the advertised node.

  * `features`: A bit vector that describes the set of protocol features that
    this node understands. We'll cover this field in more detail in Chapter XX
    on the extensibility of the Lightning Protocol. At a high level, this field
    carries a set of bits (assigned in pairs) that describes which new features
    a node understands. As an example, a node may signal that it understands
    the latest and greatest channel type, which allows peers that which
    bootstrap to the network to filter out the set of nodes they want to connect
    to.

  * `timestamp`: A timestamp which should be interpreted as a unix epoch
    formatted timestamp. This allows clients to enforce a partial ordering over
    the updates to a node's announcement. 

  * `node_id`: The `secp256k1` public key that this node announcement belongs
    to. There can only be a single `node_announcement` for a given node in the
    channel graph at any given time. As a result, a `node_announcement` can
    superseded a prior `node_announcement` for the same node if it carries a
    higher time stamp.

  * `rgb_color`: A mostly unused field that allows a node to specify an RGB
    "color" to be associated with it.

  * `alias`: A UTF-8 string to serve as the nickname for a given node. Note
    that these aliases aren't required to be globally unique, nor are they
    verified in any shape or form. As a result, they are always to be
    interpreted as being "unofficial".

  * `addresses`: A set of public internet reachable addresses that are to be
    associated with a given node. In the current version of the protocol 4
    address types are supported: IPv4 (1), IPv6 (2), Tor V2 (3), Tor V3 (4). On
    the wire, each of these address types are denoted by an integer type which
    is included in parenthesis after the address type.

##### Validation

Validating an incoming `node_announcement` is straight forward, the following
assertions should be upheld when examining a node announcement: 

  * If an existing `node_announcement` for that node is already known, then the
    `timestamp` field of a new incoming `node_announcement` MUST be greater
    than the prior one.

    * With this constraint, we enforce a forced level of "freshness".

  * If no `node_announcement` exist for the given node, then an existing
    `channel_announcement` that refernces the given node (more on that later)
    MUST already exist in one's local channel graph.

  * The included `signature` MUST be a valid ECDSA signature verified using the
    included `node_id` public key and the double-sha256 digest of the raw
    message encoding (mins the signature and frame header!) as the message.

  * All included `addresses` MUST be sorted in ascending order based on their
    address identifier.

  * The included `alias` bytes MUST be a valid UTF-8 string.

#### Channel Announcement: Structure & Validation

Next, we have the `channel_announcement`. This message is used to _announce_ a
new _public_ channel to the wider network. Note that announcing a channel is
_optional_. A channel only needs to be announced if its intended to be used for
routing by the public network. Active routing nodes may wish to announce all
their channels. However, certain nodes like mobile nodes likely don't have the
uptime or desire required to be an active routing node. As a result, these
mobile nodes (which typically use light clients to connect to the Bitcoin p2p
network), instead may have purely _unadvertised_ channels. 

##### Unadvertised Channels & The "True" Channel Graph

An unadvertised channel isn't part of the known public channel graph, but can
still be used to send/receive payments. An astute reader may now be wondering
how a channel which isn't part of the public channel graph is able to receive
payments. The solution to this problem is a set of "path finding helpers" that
we call "routing hints. As we'll see in Chapter XX on the presentation/payment
layer, invoices created by nodes with unadvertised channels will include
auxiliary information to help the sender route to them assuming the no has at
least a single channel with an existing public routing node.

Due to the existence of unadvertised channels, the _true_ size of the channel
graph (both the public and private components) is unknown. In addition, any
snapshot of the channel graph that doesn't come directly from one's own node
(via a Block Explorer or the like) is to be considered non-canonical as
updates to the graph are communicated using a system that only is able to
achieve an eventually consistent view of the channel graph.

##### Locating Channel In the Blockchain via Short Channel IDs

As mentioned earlier, the channel graph is authenticated due to its usage of
public key cryptography, as well as the Bitcoin blockchain as a spam prevention
system. In order to have a node accept a new `channel_announcement`, the
advertise must _prove_ that the channel actually exists in the Bitcoin
blockchain. This proof system adds an upfront cost to adding a new entry to the
channel graph (the on-chain fees on must pay to create the UTXO of the
channel). As a result, we mitigate spam and ensure that another node on the
network can't costless fill up the disk of an honest node with bogus channels.

Given that we need to construct a proof of the existence of a channel, a
natural question that arises is: how to we "point to" or reference a given
channel for the verifier? Given that a channel MUST be a UTXO, an initial
thought might be to first attempt to just advertise the full outpoint
(`txid:index`) of the channel. Given the outpoint of a UTXO is globally unique
one confirmed in the chain, this sounds like a good idea, however it has one
fatal flow: the verifier must maintain a full copy of the UTXO set in order to
verify channels. This works fine for full-node, but light clients which rely on
primarily PoW verification don't typically maintain a full UTXO set. As we want
to ensure we can support mobile nodes in the Lightning Network, we're forced to
find another solution.

What if rather than referencing a channel by its UTXO, we reference it based on
its "location" in the chain? In order to do this, we'll need a scheme that
allows us to "index" into a given block, then a transaction within that block,
and finally a specific output created by that transaction. Such an identifier
is described in BOLT 7 and is referred to as a: short channel ID, or `scid`.
The `scid` is used both in `channel_announcement` (and `channel_update`) as
well as within the onion encrypted routing packet included within HTLCs as we
learned in Chapter XX.

###### The Short Channel ID Identifier

Based on the information above, we have 3 piece of information we need to
encode in order to uniquely reference a given channel. As we want to very
compact representation, we'll attempt to encode the information into a _single_
integer using existing known bit packing techniques. Our integer format of
choice is an unsigned 64-bit integer, which is comprised of 8 logical bytes. 

First, the block height. Using 3 bytes (24-bits) we can encode 16777216 blocks,
which is more than double the number of blocks that are attached to the current
mainnet Bitcoin blockchain. That leaves 5 bytes left for us to encode the
transaction index and the output index respectively. We'll then use the next 3
bytes to encode the transaction index _within_ a block. This is more than
enough given that it's only possible to fix tens of thousands of transactions
in a block at current block sizes. This leaves 2 bytes left for us to encode
the output index of the channel within the transaction.

Our final `scid` format resembles: 
```
block_height (3 bytes) || transaction_index (3 bytes) || output_index (2 byes)
```

Using bit packing techniques, we first encode the most significant 3 bytes as
the block height, the next 3 bytes as the transaction index, and the least
significant 2 bytes as the output index of that creates the channel UTXO.

A short channel ID can be represented as a single integer
(`695313561322258433`) or as a more human friendly string: `632384x1568x1`.
Here we see the channel was mined in block `632384`, was the `1568` th
transaction in the block, with the channel output being found as the second
(UTXOs are zero-indexed) output produced by the transaction.

Now that we're able to succinctly defence a given channel in the chain, we can
now examine the full structure of the `channel_announcement` message, as well
as how to verify the proof-of-existence included within the message.

##### Channel Announcement Structure

A channel announcement primarily communicates two aspects:

 1. A proof that a channel exists between Node A and Node B with both nodes
 controlling the mulit-sig keys in the refracted channel output

 2. The set of capabilities of the channel (what types of HTLCs can it route,
 etc)

When describing the proof, we'll typically refer to node `1` and node `2`. Out
of the two nodes that a channel connects, the "first" node is the node that has
a "lower" public key encoding when we compare the public key of the two nodes
in compressed format hex-encoded in lexicographical order. Correspondingly, in
addition to a node public key on the network, each node should also control a
public key within the Bitcoin blockchain.

Similar to the `node_announcement` message, all included signatures of the
`channel_announcement` message should be signed/verified against the raw
encoding of the message (minus the header) that follows _after_ the final
signature (as it isn't possible for a signature to sign itself..)

With that said, a `channel_announcement` message (the edge descriptor in the
channel graph) has the following attributes:

 * `node_signature_1`: The signature of the first node over the message digest.

 * `node_signature_2`: The signature of the second node over the message
   digest.

 * `bitcoin_signature_1`: The signature of the multi-sig key (in the funding
   output) of the first node over the message digest.

 * `bitcoin_signature_2`:  The signature of the multi-sig key (in the funding
   output) of the second node over the message digest.

 * `features`: A feature bitvector that describes the set of protocol level
   features supported by this channel.

 * `chain_hash`: A 32 byte hash which is typically the genesis block hash of
   the chain the channel was opened within.

 * `short_channel_id`: The `scid` that uniquely locates the given channel
   within the blockchain.

 * `node_id_1`: The public key of the first node in the network.

 * `node_id_2`: The public key of the second node in the network.

 * `bitcoin_key_1`: The raw multi-sig key for the channel funding output for
   the first node in the network.

 * `bitcoin_key_2`: The raw multi-sig key for the channel funding output for
   the second node in the network.

##### Channel Announcement Validation

Now that we know what a `channel_announcement` contains. We can now move onto
to exactly _how_ to verify such an announcement. Verifying a channel
announcement entails verifying that:

 1. The `chain_hash` specified by the channel announcement belongs to a known,
 and supported blockchain.

 2. An _unspent_ UTXO exists in the chain as identified by the short channel
 ID.

 3. The unspent UTXO has a p2wsh multi-sig script that is composed of
 `bitcoin_key_1`, and `bitcoin_key_2`, ordered in lexicographic order.

 4. The signatures of `node_signature_1`, `node_signature_2`,
 `bitcoin_signature_1`, and `bitcoin_signature_2` are valid signatures under
 the corresponding public keys over the defined message digest.  The message
 digest is the serialization of the raw message bytes after the final
 signature. More on this in the chapter that covers wire message framing.

The channel announcement is a core component of the Lighting Network channel
graph, as it itself is an unauthenticated proofs of the existence of a channel.
As verification of channel announcement ensures that the output is _unspent_ in
order to add a new edge (a channel) to the public channel graph an upfront
chain fee must be paid. In addition to this by verifying the signatures of both
the bitcoin and node signatures, we ensure that the parties advertising the
channel are able to update the underlying multi-sig of the channel, and also
are aware of each other (all pubkeys are signed).

A channel announcement is made available to the public network only for the set
of _public_ channels that wish to be used for forwarding/routing payments. Only
a single instance of a channel announcement exists for a given channel. The
channel announcement describes the "base" of a channel on the network (it's
total capacity, location, participants, etc), but not the _shape_ of the
channel. A prospective sender needs to know bout the shape and the shape in
order to route through a channel. 

In our terminology, the "base" of a channel is fixed and cannot be changed
(only by spending the output which closes the channel). On the other hand, the
"shape" of the channel can be changed simply by updating the information that
advertise _how_ to route through the channel. When we say "how" , we refer to
what fees to use, the largest payment accepted, etc. As the channels are
bi-directional, that routing policy (how one should route through the channel)
needs to be defined in both directions. We call this routing policy the
`channel_update`, which both sides each being able to indecently update the
routing policy for "their end" of the channel.

#### Channel Update: Structure & Validation

Now that we've covered the message that advertises the base (the unchangeable
attributes that anchor a channel to the main chain) of a channel, we'll cover
the advertisement that governs the "shape" of the channel, or its routing
policy. The routing policy for a given channel announcement is referred to as
the `channel_update` for the channel.

Similar to the `channel_announcement`, then `channel_update` is authenticated
via a digital signature, to ensure that only the party that actually _owns_ the
channel is able to update the routing policy for said channel. This prevents
other parties from spoofing "fake" channel updates. Unlike
`channel_announcement`s which are created, then set it stone, a
`channel_update` can be updated at any time by either end of a channel. As
we'll see below, nodes in the network employ spam prevention and throttling
heretics to ensure the update traffic doesn't overwhelm the network

With the basics, covered, we'll now explore the precise structure of the
`channel_update` and how each of the encoded fields are used by senders to
route through a channel, and also how a forward ng node can "classify" its
channel to tend towards certain types of traffic based on its selection of the
`channel_update` parameters.

##### Anatomy of a Channel Update

The channel update is the lifeblood of the core of the routing network.
Advertising a new `channel_update` allows routing nodes to modify their fees,
the status of their channel, security parameters, and other critical routing
information.

First, we'll enumerate each field, and provide a brief description. In a later
section, we'll examine the fields that dictate a channel's directional routing
policy in more detail.

  * `signature`: A valid ECDSA signature over the `channel_update` message
    digest signed by the node that's advertising this update.

  * `chain_hash`: A 32-byte hash that denotes the chain that this channel is
    anchored in. This is typically the genesis hash of the chain.

  * `short_channel_id`: The 64-bit short channel ID that identifies which
    channel in the Channel Graph is being updated.

  * `timestamp`: A 32-bit time stamp typically interpreted as a unix epoch
   timestamp which allows an update to create a relative sequencing of their
   channel updates. An update with a higher timestamp is more recent than one
   with a lower one.

  * `message_flags`: A bitfield that's used to denotes the presence of optional
   fields.

  * `cltv_expiry_delta`: A forwarding related security parameter for the
   channel. This value denotes the minimum gap (or delta) between the incoming
   and outgoing HTLCs that are to be routed thorough this channel.

  * `htlc_minimum_msat`: A value that denotes the smallest HTLC that's
   permitted to be routed through this channel. This is expressed in
   milli-satoshis.

  * `fee_base_msat`: The _base_ fee that must be paid for all payments routed
   through this node regardless of payment size. This is expressed in
   milli-satoshis.

  * `fee_proportional_millionths`: The _proportional_fee to be paid in
   millionths of milli-satoshi. This is the denominator to be used in _fixed
   point_ arithmetic when computing the _rate_ that needs to be paid when
   routing through a channel.

  * `htlc_maximum_msat`: The largest HTLC that is permitted to be routed
   through this channel. This is expressed in milli-satoshis.

Now that we know all the fields of the `channel_update` message, we'll describe
at a high level how verification of a channel update is to be carried out. Most
importantly, the `signature` MUST be valid, and the `timestamp` MUST always be
increasing in order to invalidate prior updates.

##### Channel & Message Flags

As mentioned earlier, there're two `channel_update` messages for each direction
of the channel. Therefore, we need to be able to identify which direction of
the channel is being updated. If we have two nodes Alice, and Bob, then there
may exist a channel update in the direction of Alice -> Bob, as well as for Bob
-> Alice. In the first case, we require Alice to be the one that signs the
update (as she will be carrying the HTLC), and in the second case, we require
that Bob is the one that signs the update.

Thankfully, we can use a single bit to denote which direction is sending out
the update. Recall that in the `channel_announcement` message, we sort the node
public keys of both nodes lexicographically and refer to the "first" node as
`node_1`, and the second as `node_2`. We then carry this over to the
`channel_update` message within the `channel_flags` field. The field itself is
a bit field, with the LSB denoting the "direction" of the channel update. If
the bit is set, then this update belongs to `node_1`, otherwise to `node_2`.

##### Routing Policy Control in Channel Updates

The main purpose of the `channel_update` message is to allow nodes to be able
to update their routing policy on the network. As conditions change, nodes may
want to update aspects of their routing policy that dictate what can route
through their channels, and how the HTLC that arrive at their channels should
look like in order to be forwarded along.

First, routing fees. In the LN, routing fees are the difference between the
outgoing and incoming HTLCs of a forwarded payment. By sending out less than
they receive, routing nodes are able to profit by earning a fees for their help
to route the HTLC through the network. All HTLCs in the LN are expressed in
`msat` or milli-satoshis instead of satoshis or Bitcoin. We use `msat`, as it
allows us to charge fees for 1 sat payments. The routing fees has two
component, `fee_base_msat` and `fee_proportional_millionths`. In order to
calculate how much a payment of `N msat` should pay to route through a channel,
we use the following equation:
```
fee = fee_base_msat + (N * fee_proportional_millionths) / 1,000,000
```

This may look a bit strange at first, but all we're doing in the latter half of
the equation is computing the rate to be paid. Rather than floating point
arithmetic, we use fixed point arithmetic with 1 million as our base. This lets
us express very small amounts of fees to be paid in the `mast` unit. Let's say
we use a value of `100,000` for `fee_proportional_millionths`, this is
equivalent to a `10%` fee as `100,000/1,000,000 = 0.10` or `10%.

Another important security related parameter for a channel is the
`cltv_expiry_delta` field. This controls the delta between the incoming and
outgoing HTLCs that the routing node dictates. We refer to this as a security
parameter, as it's the of blocks between the expiry of the outgoing HTLC and
the incoming HTLC. After the outgoing HTLC expires, the routing node has
`cltv_expiry_delta` blocks to fully resolve it (sweep on timeout) before the
incoming time-lock expires. If the routing node cannot resolve the outgoing
HTLC within `cltv_expiry_delta` blocks, then a _race condition_ arises. This
race condition can result in a loss of funds, as after this period, _both_ the
outgoing and incoming time locks have expired. If the party that extended the
incoming HTLC sweeps it on chain with the timeout clause and the party that
accepted the outgoing HTLC redeems it with a pre-image, then the forwarding
nodes has lots funds as it only sent funds out and didn't receive any in.

The remaining fields will govern the availability of the channel, as well as
the amount of HTLCs that can be routed through the channel. The `channel_flags`
field is a bitfield that allows the owner of a `channel_update` to _disable_
the channel in a particular direction. This allows a node to disable routing
through the channel, but still permits it to use the channel for its own
payments. If the second LSB bit is `1`, then the channel is disabled, otherwise
it's enabled.

Next, we have the `htlc_minimum_msat` and `htlc_maximum_msat` fields. As their
name entails, this allows a node operator to restrict the _size_ of HTLCs that
are permitted to be routed through the channel. Setting these values can
recreate a sort of "routing profile" for the channel. As an example, if we set
our `htlc_minimum_msat` to a very high value, then we signal that this channel
isn't to be used to route micropayments. If we set our `htlc_maximum_msat` to a
very low value, then we signal the converse, instead that this channel is only
to be used for micropayments.

## Syncing the Channel Graph

So far we've covered the _structure_ of the channel graph, its security
properties, and various uses within the system. In this section, we'll cover
the exact protocol that's used in order to synchronize the channel graph of a
new node starting at zero. Recall the `short_channel_id` that's used to
deterministically _locate_ a channel in the underlying block chain. Notice that
as we move forward in the chain, then the absolute number of the 64-bit
representation of the `short_channel_id` increases. Using this fact, if we
start at a low blockheight say 1000, we can then use a lower bound (0 in this
case) to "seek" through the channel graph. We'll update our min and max block
height cursors in order to request all the `short_channel_id`s that exist in
that range. Once we receive that, we can then ask a  node for the precise
`channel_announcement`, `channel_update`, and `node_announcement` that allows
us to insert that edge (and the vertexes) into our local channel graph.

The protocol we just described above at a high level is used to allows nodes to
sync all the known (active) channels within the Channel Graph from a particular
block range onwards. Within the protocol itself, we use two pairs of messages
to accomplish this sync: `query_channel_range` + `reply_channel_range`, and
`query_short_channel_ids` + `reply_short_channel_ids_end`. 

Syncing is carried out in two phases, first `query_channel_range` is used to
ask a node of all the short channel IDs it knows of between two block ranges.
The responding peer then sends the set of known channel IDs in the
`reply_channel_range` message. Once the syncing peer has the latest set of
channel IDs for that block range, it uses `query_short_channel_ids` to query
for the Channel Graph announcement messages related to that short channel ID.
This process is repeated until a node arrives at the current height, after
which it knows it has all the channel graph fragments known by the responding
node.

< insert flow diagram here>

With the syncing protocol described, we'll now describe the structure of the
messages in detail.

First, the `query_channel_range` message:

  * `chain_hash` : The genesis hash of the chain we're querying for channels
    within.

  * `first_block_num`: The first block in the range we're interested in short
    channel IDs of.

  * `number_of_blocks`: The number of blocks after `first_block_num` that the
    query should encompass. This is the end point of our query.

  * `query_channel_range_tlvs: A series of optional TLV fields that are used to
    extend the query protocol.

Upon receipt of this message, the receiver will compute the starting height
`start_height = first_block_num`, and the ending height `end_height =
start_height + number_of_blocks` and use that to query its local database to
obtain a series of channel IDs whose funding transaction was created within
that block range. After querying its local database for the set of relevant
`short_channel_id`s, the responder then send the `reply_channel_range` message
which has the following structure:

  * `chain_hash`: The genesis hash of the chain that short channel IDs are
    being returned for.

  * `first_blocknum`: The first block number in the range of replies.

  * `number_of_blocks`: The number of blocks after the frist block number that
    channel IDs are being returned for.

  * `full_information`: A byte that denotes if the sending node has all
    information about this block range or not. For example, light clients may
    not have full information.

  * `len`:The number of bytes that's used to encode the `encoded_short_ids`.

  * `encoded_short_ids`: The set of encoded short channel IDs.

  * `reply_channel_range_tlvs`: A set of optional TLV records that are used to
    extend the query protocol.

In order to cut down on round trips to sync the full channel graph, and also to
reduce the bandwidth expended somewhat, the protocol support optionally
_compressing_ the set of returned channel IDs. The first bytes of the
`encoded_short_ids` field denotes what type of encoding is used. The following
encoded types are supported, with the protocol being extensible enough to allow
other types to be added later:

  * `0`: a set of uncompressed short channel IDs, in _ascending_ order

  * `1`: a set of compressed short channel IDs, in ascending order, compressed
   with zlib deflate
  

Next we move onto the query phase (which follows the "scan" phase). At this
point the syncing node will examine its local channel graph to see if there're
any items that it's missing. In other words it wants to compute the set
difference between the set of channel IDs it received and what it knows in its
local database. Any short channel IDs that it doesn't know of will be queried
for in the next step.

In order to query for short channel IDs, the syncing node will use the, the
`query_short_channel_ids` message:

  * `chain_hash`: The chain hash that denotes the genesis hash of the chain
    we're querying for channels within.

  * `len`: The number of bytes in the remaining payload.

  * `encoded_short_channel_ids`: The set of encoded short channel IDs.

  * `query_short_channel_ids_tlvs`: A set of optional TLV fields that are used
    to extend the query system.

Similar to the prior message in structure, this allows the syncing node toe
request that the responding node send over the full information of the
referenced short channel IDs, as they're missing from its local database.

Upon receiving this message, the responding node should send over all
`channel_announcement`, `channel_update`, and `node_announcement` messages (in
that order) that are related to the reference short channel IDs. Note that the
responding node can de-duplicate the `node_announcement` messages sent over in
order to reduce redundant bandwidth usage.

The `reply_short_channel_ids_end` message is sent once the responding node has
sent over all the requested information:

  * `chain_hash`: The chain hash that denotes the genesis hash of the chain
    we're querying for channels within.

  * `full_information`: A byte that denotes if the sending node has all
    information about this block range or not. For example, light clients may
    not have full information.

Once this message is received, the syncing node will now increment its cursor,
and start the scan phase again, until either it has all the channels it wants,
or arrives at the current end of the main chain.

At this point, our new node is able to sync the historical portion of the
channel graph, and validate it entirely. At this point, we now enter the
passive sync phase, rather than sync the new entries based on block height, we
can opt to instead receive the set of updates as they stream in in real-time. 

## Ongoing Channel Graph Maintenance

At this pint, we're able to sync the channel graph based on the block heights
that each of the channels were mined in. Once we have the full channel graph,
we're able to use it to make decision w.r.t which channels to open, or for path
finding to send payments. As mentioned earlier, the `channel_update` is a key
component of the system as it allows routing nodes to update their routing
policies due to changes in the market, chain conditions, or simply their own
preference.

Once we're fully send the graph, we can now being to receive any new node
announcements, channel announcements, or channel updates via the _gossip
network_. By gossip network we simply mean a set of peers that rumor updates to
the Channel Graph, in order to _eventually_ propagate their updates to the
entire network. 

It's important to note that peers on the network _won't_ immediately being to
send out updates to the channel graph sent to them by other peers. Instead, a
peer will only being to send out _real time updates_ once a special message has
been sent, the `gossip_timestamp_filter` message. This allows the sender to
communicate to the receiver that they wish to receive all updates to the
channel graph that have a `timestamp` field (remember the `channel_update`
message uses this field to sequence updates), greater than a specified value.

The `gossip_timestamp_filter` message has the following structure:

  * `chain_hash`: The genesis hash of the chain they wish to receive updates
   from.

  * `first_timestamp`: The timestamp after which, they want to receive Channel
    Graph updates for.

  * `timestamp_range`: The number of seconds after `first_timestamp` they want
    to receive updates for.

If a peer sends a `gossip_timestamp_filter` with a `first_timestamp` set to the
current unix epoch time, they they're requesting that they receive all Channel
Graph updates sequenced after that period of time. Note that if they set a
timestmap in the _past_ , then the receiver should send all updates from that
point onward, immediately after receiving the message.

This message can also be used to _disable_ receiving gossip messages from a
peer all together, by setting an initial timestamp very far in the future.
Peers on the network may use this to only sync from certain peers, or
periodically rotate our syncing peers.


